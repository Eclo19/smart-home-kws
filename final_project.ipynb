{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721a8539",
   "metadata": {},
   "source": [
    "# Key Word- Spotting Smart-Home System: ELECT_ENG 475: Machine Learning: Foundations, Applications, and Algorithms Project Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eed7e6",
   "metadata": {},
   "source": [
    "**Eric Oliveira, Justin Ansell, Vivek Matta**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c76bf85",
   "metadata": {},
   "source": [
    "**Note: This 'requirements.txt' was generated from a working conda environment on an Apple Silicon MacBook running macOS, using Miniforge3 (conda/Anaconda-compatible) as the Python distribution and Jupyter/IPython for development. The package versions reflect that specific setup and were tested in that environment; users on other platforms may need to adjust versions or install platform-specific builds accordingly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2203349e",
   "metadata": {},
   "source": [
    "Additional code / YouTube video:\n",
    "\n",
    "- [All the file processing code (GitHub)](https://github.com/Eclo19/smart-home-kws)\n",
    "- [Model training (Google Colab)](https://colab.research.google.com/drive/1zg0SPHg8Gk4uLPPREuo5_4F8gLJBUEz0?authuser=0)\n",
    "- [Documentation video (YouTube)](https://youtu.be/afolWC9hfCI?si=sEM4GCfJR_onvcgo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddc12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import os, sys\n",
    "    import numpy as np, librosa\n",
    "    import matplotlib.pyplot as plt\n",
    "    from IPython.display import Audio, display\n",
    "except ImportError:\n",
    "    print(\"To run the code in its entirity, please install the requirements.txt file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52193421",
   "metadata": {},
   "source": [
    "## Clone the GitHub Repository \n",
    "\n",
    "If this does not work, please drop the smart-home-kws directory in this script's current directory for a full demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    !git clone https://github.com/Eclo19/smart-home-kws\n",
    "    print(\"\\nSuccessfully cloned the repository.\")\n",
    "    sys.path.append(\"smart-home-kws\")\n",
    "except Exception:\n",
    "    print(\"Could not clone repository. Please add the 'smart-home-kws' directory to this working space.\")\n",
    "    print(\"   --> After adding 'smart-home-kws', please run sys.path.append('smart-home-kws')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8116c248",
   "metadata": {},
   "source": [
    "## Data Gathering and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71afae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Show a raw data point (label is the first word; 'blue' here) ---\n",
    "\n",
    "raw_m4a_path = 'smart-home-kws/Toy_Dataset/Full/blue_eric_03_full.m4a'\n",
    "raw_m4a_audio_data, sr = librosa.load(raw_m4a_path, sr=None)\n",
    "print(\"Raw audio File info:\\n\"\n",
    "      f\"    Audio file format: {raw_m4a_path.split('.')[-1]}\\n\"\n",
    "      f\"    Audio Shape       = {raw_m4a_audio_data.shape}\\n\"\n",
    "      f\"    Sample rate       = {sr}\\n\"\n",
    "      f\"    Audio dtype       = {raw_m4a_audio_data.dtype}\\n\")\n",
    "print(\"Displaying audio file:\\n\")\n",
    "display(Audio(raw_m4a_audio_data, rate=sr))\n",
    "\n",
    "t = np.arange(len(raw_m4a_audio_data)) / sr\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(t, raw_m4a_audio_data, color='black')\n",
    "plt.title(\"Example Waveform: blue_eric_03_full.m4a\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude (float32)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4082916",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Forcing project standards to the raw recording. This includes:\\n\"\n",
    "        \"    - forcing our sample rate of 22.05 kHz\\n\"\n",
    "        \"    - forcing wav format\\n\"\n",
    "        \"    - summing to mono if stereo\\n\"\n",
    "        \"    - ensuring dtype is float32\\n\"\n",
    "        \"    - normalizing\\n\")\n",
    "\n",
    "try:\n",
    "    import data_augmentation\n",
    "except ImportError:\n",
    "    print(\"Could not import Python script from the cloned Repository.\")\n",
    "else:\n",
    "    data_augmentation.VANILLA_DATA_PATH = \"smart-home-kws/Toy_Dataset/Full\"\n",
    "    data_augmentation.sanitize_vanilla_dataset()\n",
    "\n",
    "    audio_path = \"smart-home-kws/Toy_Dataset/Full/blue_eric_03_full.wav\"\n",
    "    audio_data, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "    print(\"Sanitized audio File info:\\n\\n\"\n",
    "            f\"    Audio file format: {audio_path.split('.')[-1]}\\n\"\n",
    "            f\"    Audio Shape       = {audio_data.shape}\\n\"\n",
    "            f\"    Sample rate       = {sr}\\n\"\n",
    "            f\"    Audio dtype       = {audio_data.dtype}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Show how sanitized files are chopped ---\n",
    "\n",
    "try:\n",
    "    import file_chopper\n",
    "except ImportError:\n",
    "    print(\"Could not import Python script from the cloned Repository.\")\n",
    "\n",
    "print(\n",
    "    \"The default parameters should be a good fit, but the user must choose\\n\"\n",
    "    \"their threshold and window_length to adapt to each recording.\\n\\n\"\n",
    "    \"NOTE: A prompt will show up to check/reset chopping parameters.\\n\"\n",
    "    \"If you do not wish to write the chops, press 'esc' in each prompt\\n\"\n",
    "    \"(this will throw an error).\\n\"\n",
    "    \"For demo purposes, press 1 for both prompts and write the chops.\\n\"\n",
    "    \"\\nThe parameters are already set, so simply press '1' for both prompts to generate data.\\n\"\n",
    ")\n",
    "\n",
    "file_chopper.FULL_DATA_PATH = \"smart-home-kws/Toy_Dataset/Full\"\n",
    "file_chopper.CHOPPED_DIR    = \"smart-home-kws/Toy_Dataset/Live_chops\"\n",
    "file_chopper.parse(wait=int(22050*0.15), duration=int(22050*0.45))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "live_chops_path = \"smart-home-kws/Toy_Dataset/Live_chops\"\n",
    "if os.path.isdir(live_chops_path):\n",
    "    data_augmentation.VANILLA_DATA_PATH = live_chops_path\n",
    "    data_augmentation.sanitize_vanilla_dataset()\n",
    "\n",
    "    print(\"Now displaying the first chop:\")\n",
    "    blue_1_path = os.path.join(live_chops_path, \"blue_eric_03_01.wav\")\n",
    "    audio_data, sr = librosa.load(blue_1_path, sr=None)\n",
    "    display(Audio(audio_data, rate=sr))\n",
    "\n",
    "    t = np.arange(len(audio_data)) / sr\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(t, audio_data, color=\"black\")\n",
    "    plt.title(\"Example Waveform: blue_eric_03_01.wav\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude (float32)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Live chops directory not found. Skipping...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb180c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Show data augmentation transformations ---\n",
    "print(\n",
    "    \"We split the data with wrapper.split_dataset(), but it is too costly to run here.\\n\"\n",
    "    \"Instead, we show how we expanded the training set:\\n\"\n",
    "    \"  - Filtering: taps designed with pyfda, convolved via scipy\\n\"\n",
    "    \"  - Pitch-shifting: librosa.effects\\n\\n\"\n",
    "    \"Now displaying all audio transformations applied to a training chop:\\n\"\n",
    ")\n",
    "transforms = [\n",
    "    (\"Original chop:\",       lambda x: x),\n",
    "    (\"Low-passed:\",          data_augmentation.low_pass),\n",
    "    (\"High-passed:\",         data_augmentation.high_pass),\n",
    "    (\"Band-passed:\",         data_augmentation.band_pass),\n",
    "    (\"Pitched Up:\",          data_augmentation.pitch_up),\n",
    "    (\"Pitched Down:\",        data_augmentation.pitch_down),\n",
    "    (\"Added red noise:\",     data_augmentation.add_noise),\n",
    "]\n",
    "for desc, fn in transforms:\n",
    "    print(desc)\n",
    "    transformed_audio_data = fn(audio_data)\n",
    "    display(Audio(transformed_audio_data, rate=sr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce729b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5) Show Feature Extraction ---\n",
    "print(\n",
    "    \"\\nWe build json feature dictionaries for train/val/test using \"\n",
    "    \"feature_extraction.build_feature_dict(). This calls MFCC extraction and \"\n",
    "    \"label vectorization for each point, writing (flat_mfccs, vec_label) to JSON.\"\n",
    ")\n",
    "try:\n",
    "    import feature_extraction, wrapper\n",
    "except ImportError:\n",
    "    print(\"Could not import Python script from the cloned Repository.\")\n",
    "else:\n",
    "    wrapper.force_standard_size(dirname=live_chops_path, size=39243)\n",
    "\n",
    "    audio_data, sr = librosa.load(blue_1_path, sr=None)\n",
    "    print(f\"Data length = {len(audio_data)}\")\n",
    "    mfccs = feature_extraction.extract_mfccs(audio_data=audio_data)\n",
    "    print(f\"\\nMFCCs shape: {mfccs.shape}\")\n",
    "\n",
    "    l = os.path.basename(blue_1_path).split(\"_\")[0]\n",
    "    vec_label = feature_extraction.vectorize_label(label=l)\n",
    "    print(f\"\\nParsed label = {l}, vec label:\\n{vec_label}\")\n",
    "\n",
    "    mfcc_masked = np.ma.masked_equal(mfccs, 0.0)\n",
    "    cmap = plt.cm.viridis.copy()\n",
    "    cmap.set_bad(\"white\")\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    im = plt.imshow(mfcc_masked, aspect=\"auto\", origin=\"lower\", cmap=cmap)\n",
    "    plt.title(\"Model's input - MFCC Visualization (Zero-padding shown in white)\")\n",
    "    plt.xlabel(\"Time frames\")\n",
    "    plt.ylabel(\"MFCC coefficient index\")\n",
    "    plt.colorbar(im, label=\"MFCC value\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33203431",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9c3a5",
   "metadata": {},
   "source": [
    "The full script for training the model is available and at https://colab.research.google.com/drive/1zg0SPHg8Gk4uLPPREuo5_4F8gLJBUEz0?usp=sharing. We show some basics of how the model was trained and some test data inferencing as well, but showing the full training would require too much time and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37361ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow as tf, keras\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Could not import tensorflow. This is likely due to an environment/OS mismatch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Loading the data ---\n",
    "print(\n",
    "    \"Our processed data is stored as a json feature dictionary of (x, y) tuples.\\n\"\n",
    "    \"We load it with wrapper.wrapper(dirname, augmented=False).\\n\\n\"\n",
    "    \"When loading the augmented training, validation, and test data, the shapes should be:\\n\\n\"\n",
    "    \"Train's x shape: (15561, 4928)\\n\"\n",
    "    \"Train's y shape: (15561, 9)\\n\\n\"\n",
    "    \"Val's x shape:   (485, 4928)\\n\"\n",
    "    \"Val's y shape:   (485, 9)\\n\\n\"\n",
    "    \"Test's x shape:  (473, 4928)\\n\"\n",
    "    \"Test's y shape:  (473, 9)\\n\\n\"\n",
    "    \"The split is roughly 70% train, 15% test, 15% validation (for the vanilla set).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Defining the Model ---\n",
    "L = tf.keras.layers\n",
    "\n",
    "inputs = tf.keras.Input(shape=(32, 154, 1))\n",
    "\n",
    "# Block 1\n",
    "x = L.Conv2D(64, (4, 4), activation=\"relu\", padding=\"same\")(inputs)\n",
    "x = L.BatchNormalization()(x); x = L.MaxPool2D(2)(x); x = L.SpatialDropout2D(0.1)(x)\n",
    "\n",
    "# Block 2\n",
    "x = L.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = L.BatchNormalization()(x); x = L.MaxPool2D(2)(x); x = L.SpatialDropout2D(0.1)(x)\n",
    "\n",
    "# Block 3\n",
    "x = L.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = L.BatchNormalization()(x); x = L.MaxPool2D(2)(x); x = L.SpatialDropout2D(0.2)(x)\n",
    "\n",
    "# Global pooling + Dense\n",
    "x = L.GlobalAveragePooling2D()(x)\n",
    "x = L.Dense(128, activation=\"relu\")(x)\n",
    "x = L.Dropout(0.3)(x)\n",
    "\n",
    "#Outputs\n",
    "outputs = L.Dense(9, activation=\"softmax\")(x)\n",
    "cnn = tf.keras.Model(inputs, outputs)\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c1e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Training sketch (no data provided to avoid computation) ---\n",
    "\n",
    "# Hyperparameters\n",
    "lr, batch_size, epochs = 1e-3, 50, 20\n",
    "stop_patience, lr_patience, lr_factor = 5, 2, 0.5\n",
    "\n",
    "# Compile model\n",
    "cnn.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "C = tf.keras.callbacks\n",
    "early_stop = C.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=stop_patience,\n",
    "    restore_best_weights=True, verbose=1\n",
    ")\n",
    "reduce_lr = C.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=lr_factor,\n",
    "    patience=lr_patience, min_lr=1e-6, verbose=1\n",
    ")\n",
    "checkpoint = C.ModelCheckpoint(\n",
    "    filepath=\"best_cnn_run4_padrobust-epoch{epoch:02d}-\"\n",
    "             \"valloss{val_loss:.4f}-valacc{val_accuracy:.4f}.keras\",\n",
    "    monitor=\"val_loss\", save_best_only=True, verbose=1\n",
    ")\n",
    "# Sketch of the training loop (disabled)\n",
    "if False:\n",
    "    x = y = xv = yv = None\n",
    "    history = cnn.fit(\n",
    "        x, y,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "        validation_data=(xv, yv),\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f494d44",
   "metadata": {},
   "source": [
    "## Real-Time Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7fd7cd",
   "metadata": {},
   "source": [
    "Real-time inference works in the following way:\n",
    "\n",
    "1) Record a 4s long buffer when the button is pressed\n",
    "2) Extract an input window of length 39243 (model's input size in samples) based on DSP techniques (Leaky integrator's maximum will determine the midpoint of the input window)\n",
    "3) Normalize and force zeros around the midpoint of the window, to mimic zero-padded training data\n",
    "4) Extract zeroed-out window's MFCCs and feed it to the model\n",
    "5) Get a prediction\n",
    "6) Take some action based on the prediction in the embedded device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Run Inference on Test Data ---\n",
    "\n",
    "test_path  = \"smart-home-kws/Toy_Dataset/Test Data\"\n",
    "MODEL_PATH = \"smart-home-kws/Models/best_cnn_run4_padrobust-epoch15-valloss0.0025-valacc1.0000.keras\"\n",
    "N_MFCC, T_FRAMES = feature_extraction.N_MFCC, feature_extraction.T_FRAMES\n",
    "\n",
    "# Random test point + true label\n",
    "test_points    = [f for f in os.listdir(test_path) if f.lower().endswith((\".wav\", \".m4a\", \".flac\", \".ogg\"))]\n",
    "test_filename  = np.random.choice(test_points)\n",
    "test_data_path = os.path.join(test_path, test_filename)\n",
    "true_label_str = test_filename.split(\"_\")[0]\n",
    "print(f\"Random test file: {test_filename}\")\n",
    "print(f\"True label parsed from filename: {true_label_str}\")\n",
    "\n",
    "# Label mapping dim --> string\n",
    "label_mapping = {\n",
    "    \"red\": 0, \"green\": 1, \"blue\": 2, \"white\": 3, \"off\": 4,\n",
    "    \"time\": 5, \"temperature\": 6, \"unknown\": 7, \"noise\": 8\n",
    "}\n",
    "idx_to_label = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Audio + MFCCs\n",
    "audio_data, sr = librosa.load(test_data_path)\n",
    "display(Audio(audio_data, rate=sr))\n",
    "mfccs = feature_extraction.extract_mfccs(audio_data=audio_data)\n",
    "print(\"MFCCs shape:\", mfccs.shape)\n",
    "\n",
    "# Load model\n",
    "print(f\"\\nLoading model from: {MODEL_PATH}\")\n",
    "model = keras.models.load_model(MODEL_PATH, compile=False)\n",
    "print(\"Loaded model successfully.\\n\")\n",
    "\n",
    "# Prepare input + predict\n",
    "input_matrix = mfccs.reshape(1, N_MFCC, T_FRAMES, 1).astype(np.float32)\n",
    "print(\"input_matrix shape:\", input_matrix.shape, \"dtype:\", input_matrix.dtype)\n",
    "y_pred    = model.predict(input_matrix, verbose=0)[0]\n",
    "pred_idx  = int(np.argmax(y_pred))\n",
    "pred_label = idx_to_label[pred_idx]\n",
    "\n",
    "print(\"\\nInference Result\")\n",
    "print(f\"    - True label      : {true_label_str}\")\n",
    "print(f\"    - Predicted label : {pred_label}\")\n",
    "print(\"    - Model output (softmax probabilities):\")\n",
    "print(y_pred.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53bf8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Run Inference on Pre-Recorded Window, Simulating Embedded Behavior ---\n",
    "\n",
    "audio_data, sr = librosa.load(\"smart-home-kws/Toy_Dataset/Window/full_window.wav\", sr=None)\n",
    "audio_data = audio_data.astype(np.float32)\n",
    "audio_data /= np.max(np.abs(audio_data))\n",
    "print(\"\\nInput window test:\"); display(Audio(audio_data, rate=sr))\n",
    "\n",
    "try:\n",
    "    import inference_test as it\n",
    "except ImportError:\n",
    "    print(\"Could not import Python script from the cloned Repository.\")\n",
    "else:\n",
    "    # Extract best window + zero-out edges\n",
    "    input_window        = it.plot_signal_and_loudest_window_leaky(audio_data, sr=sr, tau_ms=200)\n",
    "    input_window_zeroed = it.zero_out(input_window)\n",
    "\n",
    "    # Plot pipeline\n",
    "    t = np.arange(len(input_window)) / sr\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
    "    ax1.plot(t, input_window);        ax1.set_title(\"Extracted Input Window\"); ax1.set_ylabel(\"Amplitude\")\n",
    "    ax2.plot(t, input_window_zeroed); ax2.set_title(\"Zeroed-Out Extracted Input Window\")\n",
    "    ax2.set_xlabel(\"Time(s)\");        ax2.set_ylabel(\"Amplitude\")\n",
    "    fig.tight_layout(); plt.show()\n",
    "\n",
    "    print(\"\\nCleaned input window:\"); display(Audio(input_window_zeroed, rate=sr))\n",
    "\n",
    "    # Extract features + run inference\n",
    "    mfccs = feature_extraction.extract_mfccs(audio_data=input_window_zeroed)\n",
    "    print(\"MFCCs shape:\", mfccs.shape)\n",
    "    input_matrix = mfccs.reshape(1, N_MFCC, T_FRAMES, 1).astype(np.float32)\n",
    "    y_pred = model.predict(input_matrix, verbose=0)[0]\n",
    "    pred_idx  = int(np.argmax(y_pred))\n",
    "    pred_label = idx_to_label[pred_idx]\n",
    "\n",
    "    print(\"\\n===== Inference Result =====\")\n",
    "    print(f\"    - Predicted label : {pred_label}\\n\"\n",
    "          f\"    - Model output (softmax probabilities):\\n{y_pred.ravel()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
