{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721a8539",
   "metadata": {},
   "source": [
    "# ELECT_ENG 475: Machine Learning: Foundations, Applications, and Algorithms Project Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eed7e6",
   "metadata": {},
   "source": [
    "**Eric Oliveira, Justin Ansell, Vivek Matta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddc12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "try:\n",
    "    import numpy as np\n",
    "    from IPython.display import Audio, display\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    import librosa\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    import sys\n",
    "\n",
    "except ImportError:\n",
    "    print(\"To run the code in its entirity, please install the requirements.txt file. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get essential code and data\n",
    "\n",
    "git = False # This flag will control code and data imports\n",
    "\n",
    "try:\n",
    "    # Get our code through GitHub, if developper tools ara available.\n",
    "    !git clone https://github.com/Eclo19/smart-home-kws\n",
    "    print(\"\\n Successfully cloned the repository.\")\n",
    "    git = True\n",
    "    sys.path.append(\"smart-home-kws\")  # path relative to the notebook\n",
    "except:\n",
    "    print(\"Could not clone repository. Assumiong 'Essential_Data' is defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8116c248",
   "metadata": {},
   "source": [
    "## Data Gathering and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71afae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Show a raw data point (label is the first word in the name; 'blue' in this case.) ---\n",
    "\n",
    "# Display a full raw recording\n",
    "if git:\n",
    "    raw_m4a_path = 'smart-home-kws/Toy_Dataset/Full/blue_eric_03_full.m4a'\n",
    "    raw_m4a_audio_data, sr = librosa.load(raw_m4a_path, sr=None)\n",
    "    print(\"Raw audio File info: \\n\")\n",
    "    print(f\"    Audio file format: {raw_m4a_path.split('.')[1]}\")\n",
    "    print(f\"    Audio Shape = {raw_m4a_audio_data.shape}\")\n",
    "    print(f\"    Sample rate = {sr}\")\n",
    "    print(f\"    Audio array datatype = {raw_m4a_audio_data[0].dtype}\")\n",
    "    print(\"\\nDisplaying audio file:\")\n",
    "\n",
    "    # Display player\n",
    "    display(Audio(data=raw_m4a_audio_data, rate=sr))\n",
    "\n",
    "    # Plot waveform\n",
    "    plt.figure(figsize=(6,4))\n",
    "    t = np.arange(0, len(raw_m4a_audio_data),1)/sr\n",
    "    plt.plot(t, raw_m4a_audio_data, color='black')\n",
    "    plt.title(\"Example Waveform: blue_eric_03_full.m4a\")\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel(\"Amplitude (float32)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4082916",
   "metadata": {},
   "outputs": [],
   "source": [
    "if git:    \n",
    "\n",
    "    # Sanitize data\n",
    "    print(\"Forcing project standards to the raw recording. This includes: \\n\")\n",
    "    print(\"    - forcing our sample rate of 22.05 Khz\")\n",
    "    print(\"    - forcing wav format\")\n",
    "    print(\"    - summing to mono if stereo\")\n",
    "    print(\"    - ensuring dtype is float32\")\n",
    "    print(\"    - normalizing\")\n",
    "    \n",
    "    try: \n",
    "        import data_augmentation\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Could not import Python script from the cloned Repository.\")\n",
    "    \n",
    "    data_augmentation.VANILLA_DATA_PATH = \"smart-home-kws/Toy_Dataset/Full\"\n",
    "    data_augmentation.sanitize_vanilla_dataset()\n",
    "\n",
    "    audio_path = 'smart-home-kws/Toy_Dataset/Full/blue_eric_03_full.wav'\n",
    "    audio_data, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "    print(\"Sanitized audio File info: \\n\")\n",
    "    print(f\"    Audio file format: {audio_path.split('.')[1]}\")\n",
    "    print(f\"    Audio Shape = {audio_data.shape}\")\n",
    "    print(f\"    Sample rate = {sr}\")\n",
    "    print(f\"    Audio array datatype = {audio_data[0].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Show how a sanitized files are chopped ---\n",
    "if git:\n",
    "    try: \n",
    "        import file_chopper\n",
    "    except ImportError:\n",
    "        print(\"Could not import Python script from the cloned Repository.\")\n",
    "\n",
    "    print(\"The default parameters should be a good fit, but the user has to input their threshold\" \\\n",
    "    \"and their window_length choices. This is necessary to adapt to all recording situations.\")\n",
    "\n",
    "    print(\"\\nNOTE: A prompt will show up to check/reset the parameters for chopping. \")\n",
    "    print(\"If you do not wish to write the chops, press 'esc' in each prompt. This will throw an error.\")\n",
    "    print(\"For demo purposes, we recommend pressing 1 for both prompts and writting the chops.\")\n",
    "    file_chopper.FULL_DATA_PATH = \"smart-home-kws/Toy_Dataset/Full\"\n",
    "    file_chopper.CHOPPED_DIR = \"smart-home-kws/Toy_Dataset/Live_chops\"\n",
    "    \n",
    "    # Adapting parameters to reasonable values for this waveform\n",
    "    file_chopper.parse(wait=(int(22050*0.15)), duration=(int(22050*0.45)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "live_chops_path = 'smart-home-kws/Toy_Dataset/Live_chops'\n",
    "if os.path.isdir(live_chops_path):\n",
    "\n",
    "    #Sanitize chops again for safety and normalization\n",
    "    data_augmentation.VANILLA_DATA_PATH = live_chops_path\n",
    "    data_augmentation.sanitize_vanilla_dataset()\n",
    "\n",
    "    print(\"Now displaying the first chop:\")\n",
    "    blue_1_path = os.path.join(live_chops_path, 'blue_eric_03_01.wav')\n",
    "    audio_data, sr = librosa.load(blue_1_path, sr=None)\n",
    "    display(Audio(data=audio_data, rate=sr))\n",
    "    \n",
    "    # Plot waveform\n",
    "    plt.figure(figsize=(6,4))\n",
    "    t = np.arange(0, len(audio_data),1)/sr\n",
    "    plt.plot(t, audio_data, color='black')\n",
    "    plt.title(\"Example Waveform: blue_eric_03_01.wav\")\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel(\"Amplitude (float32)\")\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Live chops directory not found. Skipping...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb180c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Show data augmentation transformations ---\n",
    "print(\"We split the data with wrapper.split_dataset(). However, this function takes too much\" \\\n",
    "\" time and memory to run. We move on to show how we expanded the training set. \")\n",
    "print(\"For filtering, we built the taps with pyfda and used scipy to convolve the responses with the input.\")\n",
    "print(\"For pitch-shifting, we used librosa's effects.\")\n",
    "print(\"\\nNow displaying all audio transformations applied to each point in the training data:\\n\\n\")\n",
    "\n",
    "print(\"Original chop:\")\n",
    "display(Audio(data=audio_data, rate=sr))\n",
    "print(\"Low-passed:\")\n",
    "transformed_audio_data = data_augmentation.low_pass(audio_data)\n",
    "display(Audio(data=transformed_audio_data, rate=sr))\n",
    "print(\"High-passed:\")\n",
    "transformed_audio_data = data_augmentation.high_pass(audio_data)\n",
    "display(Audio(data=transformed_audio_data, rate=sr))\n",
    "print(\"Band-passed:\")\n",
    "transformed_audio_data = data_augmentation.band_pass(audio_data)\n",
    "display(Audio(data=transformed_audio_data, rate=sr))\n",
    "print(\"Pitched Up:\")\n",
    "transformed_audio_data = data_augmentation.pitch_up(audio_data)\n",
    "display(Audio(data=transformed_audio_data, rate=sr))\n",
    "print(\"Pitched Down:\")\n",
    "transformed_audio_data = data_augmentation.pitch_down(audio_data)\n",
    "display(Audio(data=transformed_audio_data, rate=sr))\n",
    "print(\"Added red noise: (low-passed white noise)\")\n",
    "transformed_audio_data = data_augmentation.add_noise(audio_data)\n",
    "display(Audio(data=transformed_audio_data, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce729b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5) Show Feature Extraction ---\n",
    "print(\"\\nWe build json feature dictionaries for our training, validation, and test data.\")\n",
    "print(\"For that, we used the feature.extraction.build_feature_dict() function.\")\n",
    "print(\"This function calls the mfcc extraction and vectorize label functions shown below\")\n",
    "print(\"for each point in the dataset and writes a tuple(flat_mfccs, vec_label) in a json dict.\")\n",
    "\n",
    "if git:\n",
    "    try:\n",
    "        import feature_extraction\n",
    "        import wrapper\n",
    "    except ImportError:\n",
    "                print(\"Could not import Python script from the cloned Repository.\")\n",
    "\n",
    "    # Force all data points to have the same time by zero padding\n",
    "    wrapper.force_standard_size(dirname=live_chops_path, size=39243)\n",
    "\n",
    "    #Get MFCCs and vectorized label\n",
    "    audio_data, sr = librosa.load(blue_1_path, sr=None)\n",
    "    print(f\"Data length = {len(audio_data)}\")\n",
    "    mfccs = feature_extraction.extract_mfccs(audio_data=audio_data)\n",
    "    print(f\"\\nMFCCs shape: {mfccs.shape}\")\n",
    "    l = 'blue_eric_03_01.wav'.split('_')[0]\n",
    "    vec_label = feature_extraction.vectorize_label(label=l)\n",
    "    print(f\"\\nParsed label = {l}, vec label: \\n{vec_label}\")\n",
    "\n",
    "    #Plot what the model actually sees\n",
    "    mfcc_masked = np.ma.masked_where(mfccs == 0.0, mfccs)\n",
    "\n",
    "    # Copy a colormap and set the \"bad\" (masked) color to something obvious\n",
    "    cmap = plt.cm.viridis.copy()\n",
    "    cmap.set_bad(color=\"white\")   # zero-padding will appear in red\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    im = plt.imshow(\n",
    "        mfcc_masked,\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        cmap=cmap\n",
    "    )\n",
    "    plt.title(f\"Model's input - MFCC Visualization (Zero-padding shown in white)\")\n",
    "    plt.xlabel(\"Time frames\")\n",
    "    plt.ylabel(\"MFCC coefficient index\")\n",
    "    plt.colorbar(im, label=\"MFCC value\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33203431",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9c3a5",
   "metadata": {},
   "source": [
    "The full script for training the model is available and at https://colab.research.google.com/drive/1zg0SPHg8Gk4uLPPREuo5_4F8gLJBUEz0?usp=sharing. We show some basics of how the model was trained and some test data inferencing as well, but showing the full training would require too much time and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Loading the data ---\n",
    "print(\"Our processed data is stored as a json feature dictionary of (x, y) tuples.\")\n",
    "print(\"We load it to runtime with wrapper.wrapper(dirname, augmented=False). \")\n",
    "print(\"When loading the augmented training, validation, and test data, the shapes should be:\\n\")\n",
    "print(\"Train's x shape: (15561, 4928)\")\n",
    "print(\"Train's y shape: (15561, 9)\")\n",
    "print(\"\\nVal's x shape: (485, 4928)\")\n",
    "print(\"Val's y shape: (485, 9)\")\n",
    "print(\"\\nTest's x shape: (473, 4928)\")\n",
    "print(\"Test's y shape: (473, 9)\")\n",
    "print(\"\\nThe split is roughly 70% for training, 15% for test and 15% for validation (for the vanilla set).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Defining the Model ---\n",
    "\n",
    "#Input\n",
    "inputs = tf.keras.Input(shape=(32, 154, 1))\n",
    "# Block 1\n",
    "x = tf.keras.layers.Conv2D(\n",
    "    64, (4, 4),\n",
    "    activation='relu',\n",
    "    padding='same'\n",
    ")(inputs)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.SpatialDropout2D(0.1)(x)\n",
    "# Block 2\n",
    "x = tf.keras.layers.Conv2D(\n",
    "    64, (3, 3),\n",
    "    activation='relu',\n",
    "    padding='same'\n",
    ")(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.SpatialDropout2D(0.1)(x)\n",
    "# Block 3\n",
    "x = tf.keras.layers.Conv2D(\n",
    "    128, (3, 3),\n",
    "    activation='relu',\n",
    "    padding='same'\n",
    ")(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.SpatialDropout2D(0.2)(x)\n",
    "# Global pooling \n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "# Dense Layers\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "#Output\n",
    "outputs = tf.keras.layers.Dense(9, activation='softmax')(x)\n",
    "# Get summary\n",
    "cnn = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "cnn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c1e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Training sketch (no data provided to avoid computation) ---\n",
    "\n",
    "# Hyperparameters used\n",
    "learning_rate = 1e-3\n",
    "batch_size = 50\n",
    "epochs = 20\n",
    "stop_patience = 5\n",
    "lr_patience = 2\n",
    "lr_reduct_factor = 0.5\n",
    "# Build model\n",
    "cnn.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=stop_patience,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=lr_reduct_factor,\n",
    "    patience=lr_patience,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"best_cnn_run4_padrobust-epoch{epoch:02d}-\"\n",
    "             \"valloss{val_loss:.4f}-valacc{val_accuracy:.4f}.keras\",\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "# Provide no data, simply show the training structure looks like\n",
    "if False:\n",
    "    x, y = None \n",
    "    xv, yv, = None\n",
    "    history = cnn.fit(\n",
    "        x,\n",
    "        y,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "        validation_data=(xv, yv),\n",
    "        verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f494d44",
   "metadata": {},
   "source": [
    "## Real-Time Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7fd7cd",
   "metadata": {},
   "source": [
    "Real-time inference works in the following way:\n",
    "\n",
    "1) Record a 4s long buffer when the button is pressed\n",
    "2) Extract an input window of length 39243 (model's input size in samples) based on DSP techniques (Leaky integrator's maximum will determine the midpoint of the input window)\n",
    "3) Normalize and force zeros around the midpoint of the window, to mimic zero-padded training data\n",
    "4) Extract zeroed-out window's MFCCs and feed it to the model\n",
    "5) Get a prediction\n",
    "6) Take some action based on the prediction in the embedded device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Run Inference on Test Data ---\n",
    "\n",
    "# Constants\n",
    "test_path  = 'Toy_Dataset/Test Data'\n",
    "MODEL_PATH = 'smart-home-kws/Models/best_cnn_run4_padrobust-epoch15-valloss0.0025-valacc1.0000.keras'\n",
    "N_MFCC   = feature_extraction.N_MFCC\n",
    "T_FRAMES = feature_extraction.T_FRAMES\n",
    "\n",
    "# Choose a random test point\n",
    "test_points = [\n",
    "    f for f in os.listdir(test_path)\n",
    "    if f.lower().endswith(('.wav', '.m4a', '.flac', '.ogg'))\n",
    "]\n",
    "test_filename = np.random.choice(test_points)\n",
    "test_data_path = os.path.join(test_path, test_filename)\n",
    "print(f\"Random test file: {test_filename}\")\n",
    "true_label_str = test_filename.split('_')[0]\n",
    "print(f\"True label parsed from filename: {true_label_str}\")\n",
    "\n",
    "# Label mapping dim --> string\n",
    "label_mapping = {\n",
    "    'red': 0, 'green': 1, 'blue': 2, 'white': 3, 'off': 4,\n",
    "    'time': 5, 'temperature': 6, 'unknown': 7, 'noise': 8\n",
    "}\n",
    "idx_to_label = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Get audio features and display audio\n",
    "audio_data, sr = librosa.load(test_data_path)\n",
    "display(Audio(data=audio_data, rate=sr))\n",
    "mfccs = feature_extraction.extract_mfccs(audio_data=audio_data)\n",
    "print(\"MFCCs shape:\", mfccs.shape)\n",
    "\n",
    "# Load model\n",
    "print(f\"\\nLoading model from: {MODEL_PATH}\")\n",
    "model = keras.models.load_model(MODEL_PATH, compile=False)\n",
    "print(\"Loaded model successfully.\\n\")\n",
    "\n",
    "# Prepare input for CNN and predict\n",
    "input_matrix = mfccs.reshape(1, N_MFCC, T_FRAMES, 1).astype(np.float32)\n",
    "print(\"input_matrix shape:\", input_matrix.shape, \"dtype:\", input_matrix.dtype)\n",
    "y_pred = model.predict(input_matrix, verbose=0)[0]\n",
    "pred_idx = int(np.argmax(y_pred))\n",
    "pred_label = idx_to_label[pred_idx]\n",
    "print(\"\\nInference Result \")\n",
    "print(f\"    -True label      : {true_label_str}\")\n",
    "print(f\"    -Predicted label : {pred_label}\")\n",
    "print(f\"    -Model output (softmax in final layer, these are the probabilities): \\n{y_pred.ravel()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53bf8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Run Inference on Pre-Recorded Window, Simulating Embedded Behavior ---\n",
    "\n",
    "audio_data, sr = librosa.load(\"Toy_Dataset/Window/full_window.wav\", sr=None)\n",
    "audio_data = audio_data.astype(np.float32)\n",
    "audio_data = audio_data / np.max(np.abs(audio_data)) # Normalize\n",
    "print(\"\\nInput window test:\")\n",
    "display(Audio(data=audio_data, rate=sr))\n",
    "try:\n",
    "    import inference_test\n",
    "except ImportError:\n",
    "            print(\"Could not import Python script from the cloned Repository.\")\n",
    "# Extract best window and plot it\n",
    "input_window = inference_test.plot_signal_and_loudest_window_leaky(\n",
    "    audio_data=audio_data,\n",
    "    sr=sr,\n",
    "    tau_ms=200\n",
    ")\n",
    "# Zero-out non-audio beginning and end (similar process as file_chopper)\n",
    "input_window_zeroed = inference_test.zero_out(input_window)\n",
    "\n",
    "# Plot audio data's pipeline\n",
    "plt.figure(figsize=(12,6))\n",
    "t = np.arange(0, len(input_window)) / sr \n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(t, input_window)\n",
    "plt.title(\"Extracted Input Window\")\n",
    "plt.xlabel(\"Time(s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(t, input_window_zeroed)\n",
    "plt.title(\"Zeroed-Out Extracted Input Window\")\n",
    "plt.xlabel(\"Time(s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show clean input\n",
    "print(\"\\nCleaned input window:\")\n",
    "display(Audio(data=input_window_zeroed, rate=sr))\n",
    "\n",
    "\n",
    "# Extract features\n",
    "mfccs = feature_extraction.extract_mfccs(audio_data=input_window_zeroed)\n",
    "print(\"MFCCs shape:\", mfccs.shape)\n",
    "\n",
    "# Prepare input for CNN and predict\n",
    "input_matrix = mfccs.reshape(1, N_MFCC, T_FRAMES, 1).astype(np.float32)\n",
    "y_pred = model.predict(input_matrix, verbose=0)[0]\n",
    "pred_idx = int(np.argmax(y_pred))\n",
    "pred_label = idx_to_label[pred_idx]\n",
    "print(\"\\n=====Inference Result =====\\n\")\n",
    "print(f\"    -Predicted label : {pred_label}\\n\")\n",
    "print(f\"    -Model output (softmax in final layer, these are the probabilities): \\n{y_pred.ravel()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
